\documentclass[final]{beamer}
\mode<presentation>
{
  \usetheme{JDM}
}

\usepackage{multirow}
\usepackage{ragged2e}
\usepackage{times}
\usepackage{amsmath,amssymb}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[orientation=landscape, size=custom, width=91, height=114, scale=1]{beamerposter}
\usepackage{array}
\usepackage{xspace}
\usepackage{xcolor}
\usepackage{epsfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{wrapfig}

\newcommand{\iqadataset}{\mbox{IQA}}
\newcommand{\thornav}{\mbox{IQA}}

\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\newcommand{\model}{$\mathcal{M}$}
\newcommand{\mmodel}{\mathcal{M}}
\newcommand{\visionin}{$\mathcal{V}$}
\newcommand{\mvisionin}{\mathcal{V}}
\newcommand{\languagein}{$\mathcal{L}$}
\newcommand{\mlanguagein}{\mathcal{L}}
\newcommand{\historyin}{$a$}
\newcommand{\mhistoryin}{a}
\newcommand{\mzero}{\textcolor{red}{\vec{0}}}

\newcommand{\navL}{$\mathcal{A}+\mathcal{L}$}  % Navigation No Vision
\newcommand{\navV}{$\mathcal{A}+\mathcal{V}$}  % Navigation No Language
\newcommand{\navA}{$\mathcal{A}$}  % Navigation Action Only
\newcommand{\qaL}{$\mathcal{L}$ \textsc{only}}  % Navigation No Vision
\newcommand{\qaV}{$\mathcal{V}$ \textsc{only}}  % Navigation No Language
\newcommand{\qaA}{$\mathcal{A}$ \textsc{only}}  % Navigation No Language

\newcommand{\good}[1]{\textcolor{blue}{\textbf{#1}}}
\newcommand{\bad}[1]{\textcolor{blue}{\textbf{#1}}}

% Poster text sizes.
\newcommand{\setblocksize}{\LARGE \centering}
\newcommand{\setsize}{\Large}
\newcommand{\setcentersize}{\LARGE}
\newcommand{\paragraphbreak}{\vspace{1cm}}

\title{Shifting the Baseline:\\Single Modality Performance on Visual Navigation \& QA}
\author{Jesse Thomason, Daniel Gordon, and Yonatan Bisk}
\institute{{\texorpdfstring{\color{blue} \bf \huge}{ } Jesse Thomason}, Daniel Gordon, and Yonatan Bisk\\University of Washington}

\date{\today}

\begin{document}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{} 
  \vfill
  
%\vspace{-270pt}  %added here to get it closer to top...
\begin{columns}[t]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{column}{.24\linewidth}    %%% COLUMN 1 %%%

\begin{block}{\setblocksize Unimodal Baselines}
%\vspace{-30pt}
	\vspace{1mm}
\justifying{\setsize
We demonstrate the strength of unimodal baselines in multimodal domains.
We argue that unimodal approaches better capture and reflect dataset biases than random or majority class baselines, and therefore provide an important comparison when assessing the performance of multimodal techniques.

}
\end{block}

\begin{block}{\setblocksize Evaluation Framework}
%\vspace{-30pt}
  \vspace{1mm}
\justifying{\setsize

We ablate benchmark models from:

\begin{enumerate}
\item \textbf{Matterport} Room-2-Room Navigation -- (\textit{Anderson et al., CVPR'18});
\item \textbf{THOR} Interactive Question Answering -- (\textit{Gordon et al., CVPR'18});
\item \textbf{EQA} Embodied Question Answering -- (\textit{Das et al., CVPR'18}).
\end{enumerate}

We define three ablations:
\vspace{-10pt}
\begin{align*}
    &\text{Full Model} & \mathrm{is} && \mmodel(\mvisionin_t, \mlanguagein, \mhistoryin_{t-1}; W) \\
    &\mathcal{A} & \mathrm{is} && \mmodel(\hspace{3pt}\mzero\hspace{2pt},\hspace{1pt} \mzero\hspace{2pt}, \mhistoryin_{t-1}; W) \\
    &\mathcal{A+V} & \mathrm{is}& & \mmodel(\mvisionin_t,\hspace{1pt} \mzero\hspace{2pt}, \mhistoryin_{t-1}; W)\\
    &\mathcal{A+L} & \mathrm{is}& & \mmodel(\hspace{3pt}\mzero\hspace{2pt}, \mlanguagein, \mhistoryin_{t-1}; W)
\end{align*}
with $\mathbfcal{A}$ction inputs, $\mathbfcal{V}$ision inputs, and $\mathbfcal{L}$anguage inputs.

At each timestep an agent receives an observation and produces an action.
\begin{equation}
    a_t \leftarrow \mmodel(\mvisionin_t, \mlanguagein, \mhistoryin_{t-1}; W)
\end{equation} 
\paragraphbreak

\begin{figure}[t]
\includegraphics[width=\columnwidth]{figures/matterport_actions_squeeze_top_big.pdf}
\centering
\end{figure}
$P(act=col | prev=row)$ and marginal action distributions in Matterport data reveal memorizable peakiness.
\paragraphbreak

\begin{figure}[t]
\includegraphics[width=\columnwidth]{figures/THOR_Question_Image2.pdf}
\centering
\end{figure}
Relevant visual observations are made after navigating to an implicit goal point in QA tasks.

}
\end{block}

\end{column}	%%% END COLUMN 1 %%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{column}{.49\linewidth}    %%% COLUMN 2 %%%

\begin{block}{\setblocksize Punchline}
%\vspace{-30pt}
  \vspace{1mm}
\justifying{\Huge
Language-only and vision-only VLN and QA models outperform published baselines and even beat their multi-modal counterparts!
}
\paragraphbreak

\justifying{\setcentersize
\begin{tabular}{p{800pt}p{150pt}}
\textbf{Recommendation for Best Practices:} While many papers ablate either language \textit{or} vision, researchers should ablate \textit{both}.
These unimodal baselines expose possible gains from single-modality biases in multimodal datasets irrespective of training and architecture details. &
\raisebox{-0.85\height}{\includegraphics[width=0.35\columnwidth]{QR.png}}
\end{tabular}

}
\end{block}

\begin{block}{\setblocksize }
%\vspace{-30pt}
  \vspace{1mm}
\justifying{\setcentersize

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/Figure1_noLabels.pdf}\\
\vspace{-5pt}\begin{small}Actions: \textbf{\texttt{F}}orward, turn \textbf{\texttt{L}}eft \& \textbf{\texttt{R}}ight, tilt \textbf{\texttt{U}}p \& \textbf{\texttt{D}}own, \textbf{\texttt{E}}nd\end{small}
\end{figure}
In the Matterport Room-2-Room task, navigating without vision can lead to sensible navigation trajectories in response to commands like ``walk past the bar and turn right''.
At $t_3$, ``forward'' is unavailable as the agent would collide with the wall, rendering the visual context for the command unnecessary.
\paragraphbreak

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figures/EQA_samples_1col_4.pdf}
\end{figure}
Qualitative results on the EQA task illuminate some unimodal biases in the data.
The language only model can pick out the most likely answer for a given question without visual context.
The vision only model finds and reports salient color and room feature as answers without being aware of the question.

}
\end{block}

\end{column}  %%% END COLUMN 2 %%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{column}{0.24\linewidth}    %%% COLUMN 3 %%%

\begin{block}{\setblocksize Unimodal Evaluation}
%\vspace{-30pt}
	\vspace{1mm}
\justifying{\setsize

Best unimodal in \textbf{bold}, {\color{blue}blue} indicates better than baseline; and $^*$ indicates better than full model.

\paragraphbreak
\textbf{Visual Navigation.}

{\setsize \large
\begin{table}
\centering
\begin{tabular}{@{}l@{\hspace{5pt}}l@{\hspace{5pt}}r@{\hspace{5pt}}r@{\hspace{13pt}}r@{\hspace{5pt}}r@{\hspace{13pt}}r@{\hspace{0pt}}}
    & & \multicolumn{2}{@{}c@{\hspace{10pt}}}{\textbf{Matterport}$\uparrow$} & \multicolumn{2}{@{}c@{\hspace{10pt}}}{\textbf{\thornav{}}$\uparrow$} & \multicolumn{1}{@{}c@{}}{\textbf{EQA}$\downarrow$}\\
    & & \multicolumn{2}{@{}c@{\hspace{10pt}}}{(\textit{\%})} & \multicolumn{2}{@{}c@{\hspace{10pt}}}{(\textit{\%})} & \multicolumn{1}{@{}c@{}}{ (\textit{m})}\\
    & \textbf{Model} & \textbf{Seen} & \textbf{Un} & \textbf{Seen} & \textbf{Un} &  \textbf{Un}\\
    \toprule
  \multirow{2}{*}{\rotatebox[origin=c]{90}{Pub.}} & Full Model & 27.1 & 19.6 & 77.7 & 18.08 & 4.17 \\ 
  & Baseline & 15.9 & 16.3 & \phantom{0}2.18 & \phantom{0}1.54 & 4.21 \\
  \cmidrule{2-7}
  \multirow{3}{*}{\rotatebox[origin=c]{90}{Uni}} 
  & \navA & 18.5 & 17.1 & \phantom{0}4.53 & \phantom{0}2.88 & 4.53  \\ 
  & \navV & 21.2 & 16.6 & \textbf{\phantom{0}35.6}  & \textbf{\phantom{0}7.50} & \textbf{$^*$4.11} \\
  & \navL & \textbf{23.0} & $^*$\textbf{22.1} & \phantom{0}4.03 & \phantom{0}3.46 & 4.64 \\
  \cmidrule[1pt]{1-7}
  $\Delta$ & Uni --  Base & \bad{\phantom{0}+7.1} & \bad{\phantom{0}+5.8} & \bad{+33.4} & \bad{\phantom{0}+5.96} & \bad{\phantom{0}-0.10} \\
  \bottomrule
\end{tabular}
\end{table}
}
Training via behavior cloning.
\paragraphbreak

\begin{table}
\centering
\begin{tabular}{@{}l@{\hspace{5pt}}l@{\hspace{5pt}}r@{\hspace{5pt}}r@{\hspace{20pt}}}
    & & \multicolumn{2}{@{}c@{}}{\textbf{Matterport}$\uparrow$} \\
    & & \multicolumn{2}{@{}c@{}}{(\textit{\%})} \\
    & \textbf{Model} & \textbf{Seen} & \textbf{Un} \\
    \toprule
    \multirow{2}{*}{\rotatebox[origin=c]{90}{Pub.}} & Full Model & 38.6 & 21.8 \\
  & Baseline   & 15.9 & 16.3 \\
  \cmidrule{2-4}  
  \multirow{3}{*}{\rotatebox[origin=c]{90}{Uni}} & \navA & \phantom{0}4.1 & \phantom{0}3.2 \\ 
  & \navV & \textbf{30.6} & 13.3 \\
  & \navL & 15.4 & \textbf{13.9} \\
  \cmidrule{2-4}
  $\Delta$ & Uni -- Base\phantom{0} & \bad{+14.7} & \phantom{0}-2.4 \\
  \bottomrule
\end{tabular}
\end{table}
Training via student forcing.
\paragraphbreak

\begin{table}
\centering
\begin{tabular}{@{}l@{\hspace{10pt}}l@{\hspace{10pt}}r@{\hspace{10pt}}r@{\hspace{10pt}}r@{\hspace{10pt}}}
    & & \multicolumn{3}{@{}c@{}}{$\mathbf{d_T}\downarrow$} \\
    & & \multicolumn{3}{@{}c@{}}{(\textit{m})} \\
    & \textbf{Model} & $T_{-10}$ & $T_{-30}$ & $T_{-50}$ \\
    \toprule
  \multirow{2}{*}{\rotatebox[origin=c]{90}{Pub.}} & Full & 0.971 & 4.17 & 8.83 \\
  & Baseline & 1.020 & 4.21 & 8.73 \\ 
  \cmidrule{2-5}
  \multirow{3}{*}{\rotatebox[origin=c]{90}{Uni}} & \navA & \textbf{$^*$0.893} & 4.53 & 9.56 \\ 
  & \navV & $^*$0.951 & \textbf{$^*$4.11} & \textbf{$^{\dagger}$8.83} \\
  & \navL & 0.987 & 4.64 & 9.51 \\
  \cmidrule{2-5}
  $\Delta$ & Uni -- Base\phantom{0} & \bad{-0.127} & \bad{-0.10} & +0.10 \\
  \bottomrule
\end{tabular}
\end{table}
EQA navigation final distance.
\paragraphbreak

\begin{table}
\begin{tabular}{@{}l@{\hspace{10pt}}l@{\hspace{10pt}}r@{\hspace{10pt}}r@{\hspace{10pt}}r@{\hspace{10pt}}}
    & & \multicolumn{3}{@{}c@{}}{$\mathbf{d_{min}}\downarrow$} \\
    & & \multicolumn{3}{@{}c@{}}{(\textit{m})} \\
    & \textbf{Model} & $T_{-10}$ & $T_{-30}$ & $T_{-50}$ \\
    \toprule
  \multirow{2}{*}{\rotatebox[origin=c]{90}{Pub.}} & Full & 0.291 & 2.43 & 6.45 \\
  & Baseline & 0.293 & 2.45 & 6.38 \\ 
  \cmidrule{2-5}
  \multirow{3}{*}{\rotatebox[origin=c]{90}{Uni}} & \navA & $^*$0.242 & 3.16 & 7.99 \\ 
  & \navV & $^*$0.287 & \textbf{2.51} & \textbf{$^*$6.44} \\
  & \navL & \textbf{$^*$0.240} & 3.19 & 7.96 \\
  \cmidrule{2-5}
  $\Delta$ & Uni -- Base\phantom{0} & \bad{-0.053} & +0.06 & +0.06 \\
  \bottomrule
\end{tabular}
\end{table}
EQA navigation closest distance.
\paragraphbreak

\textbf{Question Answering.}
\paragraphbreak 

\begin{table}
\centering
\begin{tabular}{@{}l@{\hspace{10pt}}lrr@{\hspace{10pt}}r@{\hspace{10pt}}}
    & & \multicolumn{2}{c}{\textbf{\iqadataset{}$\uparrow$}} & \multicolumn{1}{c}{\textbf{EQA $\uparrow$}} \\
    & \textbf{Model} & \textbf{Un} & \textbf{Seen} & \textbf{Un} \\
    \toprule
    \multirow{2}{*}{\rotatebox[origin=c]{90}{Pub.}} & Full Model & \phantom{+}88.3 & \phantom{+}89.3 & \phantom{+}64.0\\ 
     & Baseline & \phantom{+}41.7 &  \phantom{+}41.7 & \phantom{+}19.8 \\
    \cmidrule{2-5}
    \multirow{2}{*}{\rotatebox[origin=c]{90}{Uni}}
    & \qaV & \textbf{\phantom{+}43.5} & \textbf{\phantom{+}42.8} & \phantom{+}44.2 \\
    & \qaL & \phantom{+}41.7 &  \phantom{+}41.7 & \textbf{\phantom{+}48.8} \\
    \cmidrule[1pt]{1-5}
    $\Delta$ & Uni -- Base\phantom{0} & \bad{\phantom{0}+1.8} & \bad{\phantom{0}+1.1} & \bad{+29.0} \\
    \bottomrule
\end{tabular}
\end{table}
Question answering accuracy.

}
\end{block}

\end{column}	%%% END COLUMN 3 %%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{columns}

\end{frame}

\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Local Variables: 
%%% mode: latex
%%% TeX-PDF-mode: t

