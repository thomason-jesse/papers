
\citet{anderson:cvpr18} introduced a sequence-to-sequence model to serve as a learning baseline in the R2R task.
We formulate a similar model to encode an entire dialog history, rather than a single navigation instruction, as an initial learning baseline for the \task{} task.
The dialog history is encoded using an LSTM and used to initialize the hidden state of an LSTM decoder whose observations are visual frames from the environment, and whose outputs are actions in the environment (Figure~\ref{fig:model}).

We replace words that occur fewer than 5 times with an \texttt{UNK} token.
The resulting vocabulary sizes are 1042 language tokens in the training fold and 1181 tokens in the combined training and validation folds.
We also use special \texttt{NAV} and \texttt{ORA} tokens to preface a speaker's tokens, \texttt{TAR} to preface the target object token, and \texttt{EOS} to indicate the end of the input sequence.
During training, an embedding is learned for every token and given as input to the encoder LSTM.
For visual features, we embed the visual frame as the penultimate layer of an Imagenet-pretrained ResNet-152 model~\cite{he:cvpr16}.

When evaluating against the validation folds, we train only on the training fold.
When evaluating against the test fold, we train on the union of the training and validation folds.
We ablate the distance of dialog history encoded, and introduce a mixed planner and human supervision strategy at training time.
We hypothesize both that encoding a longer dialog history and using mixed-supervision steps will increase the amount the agent progresses towards the goal.

\begin{table}[ht]
\centering
\begin{small}
\begin{tabular}{ccccccc>{\raggedleft\arraybackslash}p{1.5cm}>{\raggedleft\arraybackslash}p{1.5cm}>{\raggedleft\arraybackslash}p{1.5cm}}
    & & \multicolumn{5}{c}{\textbf{Seq-2-Seq Inputs}} & \multicolumn{3}{c}{\textbf{Goal Progress} (m) $\uparrow$} \\
    & & & & & & $Q_{1:i-1} $ & & & \\
    \textbf{Fold} & & $V$ & $t_o$ & $A_i$ & $Q_i$ & $A_{1:i-1}$ & \textbf{Oracle} & \textbf{Navigator} & \textbf{Mixed} \\
\toprule
    \multirow{9}{*}{\rotatebox[origin=c]{90}{Val (Seen)}} & \multirow{5}{*}{\rotatebox[origin=c]{90}{Baselines}} & \multicolumn{5}{c}{\texttt{Shortest Path} Agent} & $8.29$ & $7.63$ & $9.52$ \\
    & & \multicolumn{5}{c}{\texttt{Random} Agent} & $0.42$ & $0.42$ & $0.42$ \\
    & & & & & & & $0.59$ & $0.83$ & $0.91$ \\
    & & \cblkmark & & & & & $4.12$ & $5.58$ & $5.72$ \\
    & & & \cblkmark & \cblkmark  & \cblkmark  & \cblkmark & $1.41$ & $1.43$ & $1.58$ \\
    \cmidrule{2-10}
    & \multirow{4}{*}{\rotatebox[origin=c]{90}{Ours}} & \cblkmark & \cblkmark & & & & $4.16$ & \good{$\pmb{5.71}$} & \good{$5.71$} \\
    & & \cblkmark & \cblkmark & \cblkmark & & & $4.34$ & $5.61$ & \good{$6.04$} \\
    & & \cblkmark & \cblkmark & \cblkmark & \cblkmark & & $4.28$ & $5.58$ & \good{$\pmb{6.16}$} \\
    & & \cblkmark & \cblkmark & \cblkmark & \cblkmark & \cblkmark & $\pmb{4.48}$ & $5.67$ & \good{$5.92$} \\
    \midrule
    \multirow{9}{*}{\rotatebox[origin=c]{90}{Val (Unseen)}} & \multirow{5}{*}{\rotatebox[origin=c]{90}{Baselines}} & \multicolumn{5}{c}{\texttt{Shortest Path} Agent} & $8.36$ & $7.99$ & $9.58$ \\
    & & \multicolumn{5}{c}{\texttt{Random} Agent} & $1.09$ & $1.09$ & $1.09$ \\
    & & & & & & & $0.69$ & $1.32$ & $1.07$ \\
    & & \cblkmark & & & & & $0.85$ & $1.38$ & $1.15$ \\
    & & & \cblkmark & \cblkmark & \cblkmark & \cblkmark & $1.68$ & $1.39$ & $1.64$ \\
    \cmidrule{2-10}
    & \multirow{4}{*}{\rotatebox[origin=c]{90}{Ours}} & \cblkmark & \cblkmark & & & & $0.74$ & \good{$1.33$} & $1.29$ \\
    & & \cblkmark & \cblkmark & \cblkmark & & & $1.14$ & $1.62$ & \good{$2.05$} \\
    & & \cblkmark & \cblkmark & \cblkmark & \cblkmark & & $1.11$ & $1.70$ & \good{$1.83$} \\
    & & \cblkmark & \cblkmark & \cblkmark & \cblkmark & \cblkmark & $\pmb{1.23}$ & $\pmb{1.98}$ & \good{$\pmb{2.10}$} \\
    \midrule
    \multirow{9}{*}{\rotatebox[origin=c]{90}{Test (Unseen)}} & \multirow{5}{*}{\rotatebox[origin=c]{90}{Baselines}} & \multicolumn{5}{c}{\texttt{Shortest Path} Agent} & $8.06$ & $8.48$ & $9.76$ \\
    & & \multicolumn{5}{c}{\texttt{Random} Agent} & $0.83$ & $0.83$ & $0.83$ \\
    & & & & & & & $0.13$ & $0.80$ & $0.52$ \\
    & & \cblkmark & & & & & $0.99$ & $1.56$ & $1.74$ \\
    & & & \cblkmark & \cblkmark  & \cblkmark  & \cblkmark  & $1.51$ & $1.20$ & $1.40$ \\
    \cmidrule{2-10}
    & \multirow{4}{*}{\rotatebox[origin=c]{90}{Ours}} & \cblkmark & \cblkmark & & & & $1.05$ & $1.81$ & \good{$1.90$} \\
    & & \cblkmark & \cblkmark & \cblkmark & & & $1.21$ & $2.01$ & \good{$2.05$} \\
    & & \cblkmark & \cblkmark & \cblkmark & \cblkmark & & $\pmb{1.35}$ & $1.78$ & \good{$2.27$} \\
    & & \cblkmark & \cblkmark & \cblkmark & \cblkmark & \cblkmark & $1.25$ & $\pmb{2.11}$ & \good{$\pmb{2.35}$} \\
    \bottomrule \\
\end{tabular}
\end{small}
\caption{
Average agent progress towards the goal location when trained using different path end nodes for supervision.
Among sequence-to-sequence ablations, \textbf{bold} indicates most progress across available language input, and \good{blue} indicates most progress across supervision signals.
}
\vspace{-7mm}
\label{tab:navigation}
\end{table}

\paragraph{Training.}
Given supervision from an end node $e(P^*)$, the agent infers navigation actions to form path $\hat{P}$.
We train all agents with student-forcing for 20000 iterations of batch size 100, and evaluate validation performance every 100 iterations (see the Appendix for details).
The best performance across all epochs is reported for validation folds.
At each timestep the agent executes its inferred action $\hat{a}$, and is trained using cross entropy loss against the action $a^*$ that is next along the shortest path to the end node $e(P^*)$.
Using the whole navigation path, $P^*$, as supervision rather than only the end node has been considered in other work~\cite{jain:acl19}.
At test time, the agents are trained up to the epoch that achieved the best performance on the \textit{unseen} validation fold and then evaluated (e.g., test fold evaluations are run only \textit{once} per agent).

Recall that for each \task{} instance, the path shown to the \ora{} during QA exchange $i$, $O_i$, and the path taken by the \nav{} after that exchange, $N_i$, are given.
We define the mixed supervision path $M_i$ as $N_i$ when $e(O_i)\in N_i$, and $O_i$ otherwise.
This new form of supervision has parallels to previous works on learning from imperfect or adversarial human demonstrations.
One common solution is to use imperfect human demonstrations to learn an initial policy which is then refined with Reinforcement Learning (RL)~\cite{taylor2011integrating}.
Learning performance can be improved by first assigning a confidence measure to the demonstrations and only including those demonstrations that pass a certain threshold~\cite{wang2017improving}.
While we leave the evaluation of more sophisticated RL methods to future work, the mixed supervision described above can be thought of as using a simple binary confidence heuristic to threshold the human demonstrations.

\paragraph{Baselines and Ablations.}
We compare the sequence-to-sequence agent to a full-state information shortest path agent, to a non-learning baseline, and to unimodal baselines.
The \texttt{Shortest Path} agent takes the shortest path to the supervision goal at inference time, and represents the best a learning agent could do under a given form of supervision.
The non-learning \texttt{Random} agent chooses a random heading and walks up to 5 steps forward (as in~\cite{anderson:cvpr18}).
Random baselines can be outperformed by unimodal model ablations---agents that consider only visual input, only language input, or neither---on VLN tasks~\cite{thomason:naacl19}.
So, we also compare our agent to unimodal baselines where agents have zeroed out visual features in place of the $V$ ResNet features at each decoder timestep (vision-less baseline) and/or empty language inputs to the encoder (language-less baseline).
To examine the impact of dialog history, we consider agents with access to the target object $t_o$; the last \ora{} answer $A_i$; the prefacing \nav{} question $Q_i$; and the full dialog history (Figure~\ref{fig:model}).

\paragraph{Results.}
Table~\ref{tab:navigation} shows agent performances given different forms of supervision.
We ran paired $t$-tests between all model ablations within each supervision paradigm and across paradigms, and applied the Benjamini--Yekutieli procedure to control the false discovery rate (details in the Appendix).

Using all dialog history significantly outperforms unimodal ablations in \textit{unseen} environments.
The \texttt{Shortest Path} agent performance with \nav{} supervision $N_i$ approximates human performance on \task{}, because $e(N_i)$ is the node reached by the human \nav{} after QA exchange $i$ during data collection.
The sequence-to-sequence models establish an initial, multimodal baseline for \task{}, with headroom remaining compared to human performance, especially in \textit{unseen} environments.
Using all dialog history, rather than just the last question or question-answer exchange, is needed to achieve statistically significantly better performance than using the target object alone in \textit{unseen} test environments.
This supports our hypothesis that dialog history is beneficial for understanding the context of the latest navigation instruction $A_i$.
Models trained with mixed supervision always statistically significantly outperform those trained with oracle or navigator supervision.
This supports our hypothesis that using human demonstrations only when they appear trustworthy increases agent progress towards the goal.
